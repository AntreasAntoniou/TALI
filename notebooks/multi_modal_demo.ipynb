{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f7a11f98550>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from rich import print\n",
    "from rich.traceback import install\n",
    "\n",
    "import datasets\n",
    "\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ! pip install -e ../\n",
    "# ! mamba install -c conda-forge itables pandas -y\n",
    "# ! pip install pytorchvideo\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from math import floor\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from hydra_zen import builds, instantiate\n",
    "from PIL import Image\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision.transforms import Compose, RandomCrop, Resize, ToTensor\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from traitlets import default\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import datasets\n",
    "from tali_wit.data import (\n",
    "    AnyModalSample,\n",
    "    dataclass_collate,\n",
    "    default_image_transforms,\n",
    "    ModalityTypes,\n",
    "    select_subtitles_between_timestamps,\n",
    "    TALIDataset,\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.data_plus import TALIBase\n",
    "from tali_wit.data import (\n",
    "    AnyModalSample,\n",
    "    dataclass_collate,\n",
    "    default_image_transforms,\n",
    "    ModalityTypes,\n",
    "    get_base_modality,\n",
    "    select_subtitles_between_timestamps,\n",
    ")\n",
    "from rich import print\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "dataset_show = TALIBase(\n",
    "    set_name=\"train\",\n",
    "    tali_dataset_dir=\"/data_fast/tali-v-3-4/\",\n",
    "    modality_list=[\n",
    "        ModalityTypes.wit_image.value,\n",
    "        ModalityTypes.wit_caption.value,\n",
    "        ModalityTypes.wit_title.value,\n",
    "        ModalityTypes.wit_main_body.value,\n",
    "        ModalityTypes.youtube_title.value,\n",
    "        ModalityTypes.youtube_image.value,\n",
    "        ModalityTypes.youtube_video.value,\n",
    "        ModalityTypes.youtube_subtitles.value,\n",
    "        ModalityTypes.youtube_audio.value,\n",
    "        ModalityTypes.youtube_description.value,\n",
    "    ],\n",
    "    rng_seed=42,\n",
    "    top_k_tali=10,\n",
    "    image_size=224,\n",
    "    num_video_frames=10,\n",
    "    num_audio_frames=16000 * 2,\n",
    "    clip_duration_in_seconds=3,\n",
    "    deterministic_sampling=True,\n",
    "    dummy_batch_mode=False,\n",
    "    image_text_model_name=\"openai/clip-vit-base-patch16\",\n",
    "    audio_model_name=\"openai/whisper-base\",\n",
    "    use_model_preprocessing=False,\n",
    "    num_samples_per_episode=num_samples,\n",
    "    total_num_samples=1000000,\n",
    "    cache_generated_samples_in_memory=True,\n",
    "    cache_num_samples=10,\n",
    ")\n",
    "\n",
    "dataset_predict = TALIBase(\n",
    "    set_name=\"train\",\n",
    "    tali_dataset_dir=\"/data_fast/tali-v-3-4/\",\n",
    "    modality_list=[\n",
    "        ModalityTypes.wit_image.value,\n",
    "        ModalityTypes.wit_caption.value,\n",
    "        ModalityTypes.wit_title.value,\n",
    "        ModalityTypes.wit_main_body.value,\n",
    "        ModalityTypes.youtube_title.value,\n",
    "        ModalityTypes.youtube_image.value,\n",
    "        ModalityTypes.youtube_video.value,\n",
    "        ModalityTypes.youtube_subtitles.value,\n",
    "        ModalityTypes.youtube_audio.value,\n",
    "        ModalityTypes.youtube_description.value,\n",
    "    ],\n",
    "    rng_seed=42,\n",
    "    top_k_tali=10,\n",
    "    image_size=224,\n",
    "    num_video_frames=10,\n",
    "    num_audio_frames=16000 * 2,\n",
    "    clip_duration_in_seconds=3,\n",
    "    deterministic_sampling=True,\n",
    "    dummy_batch_mode=False,\n",
    "    image_text_model_name=\"openai/clip-vit-base-patch16\",\n",
    "    audio_model_name=\"openai/whisper-base\",\n",
    "    use_model_preprocessing=True,\n",
    "    num_samples_per_episode=num_samples,\n",
    "    total_num_samples=1000000,\n",
    "    cache_generated_samples_in_memory=True,\n",
    "    cache_num_samples=10,\n",
    ")\n",
    "\n",
    "# sample = dataset[0]\n",
    "\n",
    "# for k, v in sample.items():\n",
    "#     print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.decorators import configurable\n",
    "from tali_wit.utils import get_logger, load_json, save_json\n",
    "from tali_wit.models import ModalityConfig\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "cache_dir = \"/data_fast/.demo/cache\"\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def update_modality_panel(modality_type):\n",
    "    if modality_type == \"text\":\n",
    "        gr.update(children=[gr.inputs.Textbox(label=\"Text\")])\n",
    "    elif modality_type == \"image\":\n",
    "        gr.update(children=[gr.inputs.Image(label=\"Image\")])\n",
    "    elif modality_type == \"video\":\n",
    "        gr.update(children=[gr.inputs.Video(label=\"Video\")])\n",
    "    elif modality_type == \"audio\":\n",
    "        gr.update(children=[gr.inputs.Audio(label=\"Audio\")])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality type: {modality_type}\")\n",
    "\n",
    "\n",
    "def video_array_to_video_file(i, video_array):\n",
    "    video_path = f\"{cache_dir}/{i}.mp4\"\n",
    "\n",
    "    if not pathlib.Path(video_path).parent.exists():\n",
    "        pathlib.Path(video_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torchvision.io.write_video(video_path, video_array, fps=3)\n",
    "    return video_path\n",
    "\n",
    "\n",
    "def audio_array_to_audio_file(i, audio_array):\n",
    "    audio_path = f\"{cache_dir}/{i}.mp3\"\n",
    "\n",
    "    if not pathlib.Path(audio_path).parent.exists():\n",
    "        pathlib.Path(audio_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    torchaudio.save(audio_path, audio_array.view(-1).unsqueeze(0), 16000)\n",
    "\n",
    "    return audio_path\n",
    "\n",
    "\n",
    "def fetch_random_samples(button):\n",
    "    mapper_dict = {\n",
    "        \"wikipedia_caption_text\": \"wit-caption\",\n",
    "        \"wikipedia_caption_image\": \"wit-images\",\n",
    "        \"youtube_content_video\": \"tali-videos\",\n",
    "        \"youtube_content_audio\": \"tali-audio\",\n",
    "        \"youtube_random_video_sample_image\": \"tali-images\",\n",
    "        \"youtube_description_text\": \"tali-description\",\n",
    "        \"youtube_title_text\": \"tali-title\",\n",
    "        \"youtube_subtitle_text\": \"tali-subtitles\",\n",
    "    }\n",
    "    reverse_mapper_dict = {v: k for k, v in mapper_dict.items()}\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    sample = dataset[idx]\n",
    "    images = {\"wit-images\": [], \"tali-images\": []}\n",
    "    videos = {\"tali-videos\": []}\n",
    "    audio = {\"tali-audio\": []}\n",
    "    text = {\n",
    "        \"wit-caption\": [],\n",
    "        \"tali-title\": [],\n",
    "        \"tali-description\": [],\n",
    "        \"tali-subtitles\": [],\n",
    "    }\n",
    "\n",
    "    for key in videos:\n",
    "        for i in range(num_samples):\n",
    "            video_array = sample[reverse_mapper_dict[key]][i].permute(\n",
    "                0, 2, 3, 1\n",
    "            )\n",
    "            video_path = video_array_to_video_file(i, video_array)\n",
    "\n",
    "            videos[key].append(gr.update(value=video_path))\n",
    "\n",
    "    for key in audio:\n",
    "        for i in range(num_samples):\n",
    "            audio_array = sample[reverse_mapper_dict[key]][i]\n",
    "            audio_path = audio_array_to_audio_file(i, audio_array)\n",
    "            audio[key].append(gr.update(value=audio_path))\n",
    "\n",
    "    for key in images:\n",
    "        for i in range(num_samples):\n",
    "            # print(f\"{key} {i} {sample[reverse_mapper_dict[key]][i]}\")\n",
    "            images[key].append(\n",
    "                gr.update(\n",
    "                    value=np.array(\n",
    "                        sample[reverse_mapper_dict[key]][i].permute(1, 2, 0)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    for key in text:\n",
    "        for i in range(num_samples):\n",
    "            text[key].append(\n",
    "                gr.update(value=sample[reverse_mapper_dict[key]][i])\n",
    "            )\n",
    "\n",
    "    image_list = [[entry for entry in item] for item in images.values()]\n",
    "    image_list = [item for sublist in image_list for item in sublist]\n",
    "    video_list = [[entry for entry in item] for item in videos.values()]\n",
    "    video_list = [item for sublist in video_list for item in sublist]\n",
    "    audio_list = [[entry for entry in item] for item in audio.values()]\n",
    "    audio_list = [item for sublist in audio_list for item in sublist]\n",
    "    text_list = [[entry for entry in item] for item in text.values()]\n",
    "    text_list = [item for sublist in text_list for item in sublist]\n",
    "\n",
    "    return *image_list, *video_list, *audio_list, *text_list\n",
    "\n",
    "\n",
    "def rank_by_random(text):\n",
    "    mapper_dict = {\n",
    "        \"wikipedia_caption_text\": \"wit-caption\",\n",
    "        \"wikipedia_caption_image\": \"wit-images\",\n",
    "        \"youtube_content_video\": \"tali-videos\",\n",
    "        \"youtube_content_audio\": \"tali-audio\",\n",
    "        \"youtube_random_video_sample_image\": \"tali-images\",\n",
    "        \"youtube_description_text\": \"tali-description\",\n",
    "        \"youtube_title_text\": \"tali-title\",\n",
    "        \"youtube_subtitle_text\": \"tali-subtitles\",\n",
    "    }\n",
    "    reverse_mapper_dict = {v: k for k, v in mapper_dict.items()}\n",
    "    sample = dataset.mem_cache[-1]\n",
    "\n",
    "    images = {\"wit-images\": [], \"tali-images\": []}\n",
    "    videos = {\"tali-videos\": []}\n",
    "    audio = {\"tali-audio\": []}\n",
    "    text = {\n",
    "        \"wit-caption\": [],\n",
    "        \"tali-title\": [],\n",
    "        \"tali-description\": [],\n",
    "        \"tali-subtitles\": [],\n",
    "    }\n",
    "    shuffle_idx = np.random.permutation(num_samples)\n",
    "    for key in videos:\n",
    "        for i in shuffle_idx:\n",
    "            video_array = sample[reverse_mapper_dict[key]][i].permute(\n",
    "                0, 2, 3, 1\n",
    "            )\n",
    "            video_path = video_array_to_video_file(i, video_array)\n",
    "\n",
    "            videos[key].append(gr.update(value=video_path))\n",
    "\n",
    "    for key in audio:\n",
    "        for i in shuffle_idx:\n",
    "            audio_array = sample[reverse_mapper_dict[key]][i]\n",
    "            audio_path = audio_array_to_audio_file(i, audio_array)\n",
    "            audio[key].append(gr.update(value=audio_path))\n",
    "\n",
    "    for key in images:\n",
    "        for i in shuffle_idx:\n",
    "            # print(f\"{key} {i} {sample[reverse_mapper_dict[key]][i]}\")\n",
    "            images[key].append(\n",
    "                gr.update(\n",
    "                    value=np.array(\n",
    "                        sample[reverse_mapper_dict[key]][i].permute(1, 2, 0)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    for key in text:\n",
    "        for i in shuffle_idx:\n",
    "            text[key].append(\n",
    "                gr.update(value=sample[reverse_mapper_dict[key]][i])\n",
    "            )\n",
    "\n",
    "    image_list = [[entry for entry in item] for item in images.values()]\n",
    "    image_list = [item for sublist in image_list for item in sublist]\n",
    "    video_list = [[entry for entry in item] for item in videos.values()]\n",
    "    video_list = [item for sublist in video_list for item in sublist]\n",
    "    audio_list = [[entry for entry in item] for item in audio.values()]\n",
    "    audio_list = [item for sublist in audio_list for item in sublist]\n",
    "    text_list = [[entry for entry in item] for item in text.values()]\n",
    "    text_list = [item for sublist in text_list for item in sublist]\n",
    "\n",
    "    return *image_list, *video_list, *audio_list, *text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model from demo_weights/checkpoints/ckpt_22015\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model from demo_weights/checkpoints/ckpt_22015\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tali_wit.utils import download_model_with_name\n",
    "from accelerate import Accelerator\n",
    "import yaml\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "path_dict = download_model_with_name(hf_cache_dir=\"demo_weights/\", \n",
    "                                     model_name=\"ckpt_22015\", \n",
    "                                     hf_repo_path=\"Antreas/tali-2-tali_omni_base_patch16_224-wit_tali_image_text_audio_video_dataset-2306\")\n",
    "\n",
    "with open(path_dict[\"config_filepath\"], \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model = instantiate(config[\"model\"])\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "print(f\"Loading model from {path_dict['root_filepath']}\")\n",
    "\n",
    "accelerator.load_state(path_dict[\"root_filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tali_wit.data_plus import generate_hierarchical_data_dict\n",
    "\n",
    "# fetch_samples = dataset_predict[0]\n",
    "# sample = dataset_predict.mem_cache[-1]\n",
    "# hierarchical_sample = generate_hierarchical_data_dict(sample)\n",
    "# hierarchical_sample = accelerator.prepare(hierarchical_sample)\n",
    "# for key, value in hierarchical_sample.items():\n",
    "#     for subkey, subvalue in value.items():\n",
    "#         hierarchical_sample[key][subkey] = accelerator.prepare(subvalue)\n",
    "#         # print(hierarchical_sample[key][subkey])\n",
    "\n",
    "# output_dict = model.forward(hierarchical_sample)\n",
    "\n",
    "# def get_similarities_with_source(source_name):\n",
    "#     filter_dict = {}\n",
    "#     for key in output_dict:\n",
    "#         if \"similarities\" in key and source_name in key:\n",
    "#             if source_name in key.split(\"_to_\")[1]:\n",
    "#                 new_key = f\"{key.split('_to_')[1]}_to_{key.split('_to_')[0]}\"\n",
    "#                 filter_dict[new_key] = output_dict[key].permute(1, 0)\n",
    "#             else:\n",
    "#                 filter_dict[key] = output_dict[key]\n",
    "#     return filter_dict\n",
    "                \n",
    "\n",
    "# test_output = get_similarities_with_source(\"wikipedia_caption_text\")\n",
    "\n",
    "# for key in test_output:\n",
    "#     print(key, test_output[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank_by_text(text):\n",
    "#     mapper_dict = {\n",
    "#         \"wikipedia_caption_text\": \"wit-caption\",\n",
    "#         \"wikipedia_caption_image\": \"wit-images\",\n",
    "#         \"youtube_content_video\": \"tali-videos\",\n",
    "#         \"youtube_content_audio\": \"tali-audio\",\n",
    "#         \"youtube_random_video_sample_image\": \"tali-images\",\n",
    "#         \"youtube_description_text\": \"tali-description\",\n",
    "#         \"youtube_title_text\": \"tali-title\",\n",
    "#         \"youtube_subtitle_text\": \"tali-subtitles\",\n",
    "#     }\n",
    "#     reverse_mapper_dict = {v: k for k, v in mapper_dict.items()}\n",
    "#     sample = dataset.mem_cache[-1]\n",
    "\n",
    "#     images = {\"wit-images\": [], \"tali-images\": []}\n",
    "#     videos = {\"tali-videos\": []}\n",
    "#     audio = {\"tali-audio\": []}\n",
    "#     text = {\n",
    "#         \"wit-caption\": [],\n",
    "#         \"tali-title\": [],\n",
    "#         \"tali-description\": [],\n",
    "#         \"tali-subtitles\": [],\n",
    "#     }\n",
    "    \n",
    "#     output_dict = model.forward(sample)\n",
    "    \n",
    "#     shuffle_idx = np.random.permutation(num_samples)\n",
    "#     for key in videos:\n",
    "#         for i in shuffle_idx:\n",
    "#             video_array = sample[reverse_mapper_dict[key]][i].permute(\n",
    "#                 0, 2, 3, 1\n",
    "#             )\n",
    "#             video_path = video_array_to_video_file(i, video_array)\n",
    "\n",
    "#             videos[key].append(gr.update(value=video_path))\n",
    "\n",
    "#     for key in audio:\n",
    "#         for i in shuffle_idx:\n",
    "#             audio_array = sample[reverse_mapper_dict[key]][i]\n",
    "#             audio_path = audio_array_to_audio_file(i, audio_array)\n",
    "#             audio[key].append(gr.update(value=audio_path))\n",
    "\n",
    "#     for key in images:\n",
    "#         for i in shuffle_idx:\n",
    "#             # print(f\"{key} {i} {sample[reverse_mapper_dict[key]][i]}\")\n",
    "#             images[key].append(\n",
    "#                 gr.update(\n",
    "#                     value=np.array(\n",
    "#                         sample[reverse_mapper_dict[key]][i].permute(1, 2, 0)\n",
    "#                     )\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     for key in text:\n",
    "#         for i in shuffle_idx:\n",
    "#             text[key].append(\n",
    "#                 gr.update(value=sample[reverse_mapper_dict[key]][i])\n",
    "#             )\n",
    "\n",
    "#     image_list = [[entry for entry in item] for item in images.values()]\n",
    "#     image_list = [item for sublist in image_list for item in sublist]\n",
    "#     video_list = [[entry for entry in item] for item in videos.values()]\n",
    "#     video_list = [item for sublist in video_list for item in sublist]\n",
    "#     audio_list = [[entry for entry in item] for item in audio.values()]\n",
    "#     audio_list = [item for sublist in audio_list for item in sublist]\n",
    "#     text_list = [[entry for entry in item] for item in text.values()]\n",
    "#     text_list = [item for sublist in text_list for item in sublist]\n",
    "\n",
    "#     return *image_list, *video_list, *audio_list, *text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    source = gr.Markdown(\"### Input Source\", interactive=False)\n",
    "    description_source_modality = gr.Markdown(\n",
    "        \"Select the type of input modality you want to use to rank the results from the tabs\",\n",
    "        interactive=False,\n",
    "    )\n",
    "    description_source_type = gr.Markdown(\n",
    "        \"Once you do, next, choose the way you wish to input the data. You can either upload a file, use your webcam, or draw on the canvas.\",\n",
    "        interactive=False,\n",
    "    )\n",
    "    description_rank = gr.Markdown(\n",
    "        \"Once you have selected a source modality, then selected an input source, and finally uploaded your data, click the Rank button\",\n",
    "        interactive=False,\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            with gr.Tab(\"text\"):\n",
    "                with gr.Tab(\"generic\"):\n",
    "                    generic_text = gr.Textbox(label=\"Text\")\n",
    "                    rank_by_generic_text_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"caption\"):\n",
    "                    caption_text = gr.Textbox(label=\"Caption\")\n",
    "                    rank_by_caption_text_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"title\"):\n",
    "                    title_text = gr.Textbox(label=\"Title\")\n",
    "                    rank_by_title_text_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"description\"):\n",
    "                    description_text = gr.Textbox(label=\"Description\")\n",
    "                    rank_by_description_text_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"subtitles\"):\n",
    "                    subtitles_text = gr.Textbox(label=\"Subtitles\")\n",
    "                    rank_by_subtitles_text_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "\n",
    "            with gr.Tab(\"image\"):\n",
    "                with gr.Tab(\"upload-image\"):\n",
    "                    uploaded_image = gr.Image(label=\"Image\")\n",
    "                    rank_by_uploaded_image_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"camera\"):\n",
    "                    recorded_image = gr.Image(source=\"webcam\", streaming=True)\n",
    "                    rank_by_recorded_image_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"draw\"):\n",
    "                    drawn_image = gr.Image(source=\"canvas\")\n",
    "                    rank_by_drawn_image_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "\n",
    "            with gr.Tab(\"video\"):\n",
    "                with gr.Tab(\"upload-video\"):\n",
    "                    upload_video = gr.Video(label=\"Video\")\n",
    "                    rank_by_upload_video_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"camera\"):\n",
    "                    camera_video = gr.Video(source=\"webcam\")\n",
    "                    rank_by_camera_video_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "\n",
    "            with gr.Tab(\"audio\"):\n",
    "                with gr.Tab(\"upload-audio\"):\n",
    "                    upload_audio = gr.Audio(label=\"Audio\")\n",
    "                    rank_by_audio_upload_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "                with gr.Tab(\"microphone\"):\n",
    "                    mic_audio = gr.Audio(source=\"microphone\")\n",
    "                    rank_by_audio_mic_button = gr.Button(\n",
    "                        \"Rank Results\", label=\"Rank Results\"\n",
    "                    )\n",
    "\n",
    "    images = {\"wit-images\": [], \"tali-images\": []}\n",
    "    videos = {\"tali-videos\": []}\n",
    "    audio = {\"tali-audio\": []}\n",
    "    text = {\n",
    "        \"wit-caption\": [],\n",
    "        \"tali-title\": [],\n",
    "        \"tali-description\": [],\n",
    "        \"tali-subtitles\": [],\n",
    "    }\n",
    "\n",
    "    source_content = gr.Markdown(\n",
    "        \"Fetch some samples to rank, by clicking the button below! 🖼️ 🎦 🔊 📝\",\n",
    "        interactive=False,\n",
    "    )\n",
    "\n",
    "    fetch_random_samples_button = gr.Button(\n",
    "        \"Fetch Random TALI Samples\", label=\"Fetch Random Samples\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            for key in images:\n",
    "                with gr.Tab(key):\n",
    "                    for i in range(num_samples):\n",
    "                        images[key].append(gr.Image())\n",
    "\n",
    "        with gr.Column():\n",
    "            for key in videos:\n",
    "                with gr.Tab(key):\n",
    "                    for i in range(num_samples):\n",
    "                        videos[key].append(gr.Video())\n",
    "\n",
    "        with gr.Column():\n",
    "            for key in audio:\n",
    "                with gr.Tab(key):\n",
    "                    for i in range(num_samples):\n",
    "                        audio[key].append(gr.Audio())\n",
    "\n",
    "        with gr.Column():\n",
    "            for key in text:\n",
    "                with gr.Tab(key):\n",
    "                    for i in range(num_samples):\n",
    "                        text[key].append(gr.Textbox(lines=3))\n",
    "\n",
    "        # unpack the lists of lists into a single list, collapse the list of lists into a single list\n",
    "        image_list = [[entry for entry in item] for item in images.values()]\n",
    "        image_list = [item for sublist in image_list for item in sublist]\n",
    "        video_list = [[entry for entry in item] for item in videos.values()]\n",
    "        video_list = [item for sublist in video_list for item in sublist]\n",
    "        audio_list = [[entry for entry in item] for item in audio.values()]\n",
    "        audio_list = [item for sublist in audio_list for item in sublist]\n",
    "        text_list = [[entry for entry in item] for item in text.values()]\n",
    "        text_list = [item for sublist in text_list for item in sublist]\n",
    "\n",
    "        fetch_random_samples_button.click(\n",
    "            fetch_random_samples,\n",
    "            [fetch_random_samples_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )\n",
    "\n",
    "        # rank_by_audio_mic_button.click(\n",
    "        #     rank_by_audio,\n",
    "        #     [rank_by_audio_mic_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "        # rank_by_audio_upload_button.click(\n",
    "        #     rank_by_audio,\n",
    "        #     [rank_by_audio_upload_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "\n",
    "        # rank_by_uploaded_image_button.click(\n",
    "        #     rank_by_image,\n",
    "        #     [rank_by_uploaded_image_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "        # rank_by_drawn_image_button.click(\n",
    "        #     rank_by_image,\n",
    "        #     [rank_by_drawn_image_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "        # rank_by_recorded_image_button.click(\n",
    "        #     rank_by_image,\n",
    "        #     [rank_by_recorded_image_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "\n",
    "        # rank_by_upload_video_button.click(\n",
    "        #     rank_by_video,\n",
    "        #     [rank_by_upload_video_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "        # rank_by_camera_video_button.click(\n",
    "        #     rank_by_video,\n",
    "        #     [rank_by_camera_video_button],\n",
    "        #     [*image_list, *video_list, *audio_list, *text_list],\n",
    "        # )\n",
    "\n",
    "        rank_by_generic_text_button.click(\n",
    "            rank_by_random,\n",
    "            [rank_by_generic_text_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )\n",
    "        rank_by_caption_text_button.click(\n",
    "            rank_by_random,\n",
    "            [rank_by_caption_text_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )\n",
    "        rank_by_title_text_button.click(\n",
    "            rank_by_random,\n",
    "            [rank_by_title_text_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )\n",
    "        rank_by_description_text_button.click(\n",
    "            rank_by_random,\n",
    "            [rank_by_description_text_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )\n",
    "        rank_by_subtitles_text_button.click(\n",
    "            rank_by_random,\n",
    "            [rank_by_subtitles_text_button],\n",
    "            [*image_list, *video_list, *audio_list, *text_list],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    }
   ],
   "source": [
    "demo.queue(concurrency_count=8)\n",
    "demo.launch(share=True, debug=True, enable_queue=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8be86470e38501aa34785446b144512d347203ba2fe09ff4115a3f561b2ac78b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
