{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fc973b277f0>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from rich import print\n",
    "from rich.traceback import install\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ! pip install -e ../\n",
    "# ! mamba install -c conda-forge itables pandas -y\n",
    "# ! pip install pytorchvideo\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from math import floor\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from hydra_zen import builds, instantiate\n",
    "from PIL import Image\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision.transforms import Compose, RandomCrop, Resize, ToTensor\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from traitlets import default\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import datasets\n",
    "from tali_wit.data import (\n",
    "    AnyModalSample,\n",
    "    dataclass_collate,\n",
    "    default_image_transforms,\n",
    "    ModalityTypes,\n",
    "    select_subtitles_between_timestamps,\n",
    "    TALIDataset,\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.decorators import configurable\n",
    "from tali_wit.utils import get_logger, load_json, save_json\n",
    "from tali_wit.models import ModalityConfig\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.data_plus import TALIBaseDemoTransform, TALIBaseTransformConfig\n",
    "\n",
    "dataset_dir = \"/data_fast/tali-v-3-4/\"\n",
    "\n",
    "transform = TALIBaseDemoTransform(\n",
    "    config=TALIBaseTransformConfig(\n",
    "        root_filepath=dataset_dir,\n",
    "        modality_list=[\n",
    "            ModalityTypes.wit_image.value,\n",
    "            ModalityTypes.wit_caption.value,\n",
    "            ModalityTypes.wit_title.value,\n",
    "            ModalityTypes.wit_main_body.value,\n",
    "            ModalityTypes.youtube_image.value,\n",
    "            ModalityTypes.youtube_video.value,\n",
    "            ModalityTypes.youtube_subtitles.value,\n",
    "            ModalityTypes.youtube_audio.value,\n",
    "            ModalityTypes.youtube_description.value,\n",
    "        ],\n",
    "        rng_seed=42,\n",
    "        top_k_tali=10,\n",
    "        image_size=224,\n",
    "        num_video_frames=200,\n",
    "        num_audio_frames=160000,\n",
    "        clip_duration_in_seconds=10.0,\n",
    "        deterministic_sampling=True,\n",
    "    )\n",
    ")\n",
    "train_dataset = datasets.load_from_disk(\n",
    "   dataset_dir + \"train-set\" # type: ignore\n",
    ")\n",
    "train_dataset = train_dataset.with_transform(transform)\n",
    "val_dataset = datasets.load_from_disk(\n",
    "    dataset_dir + \"val-set\" # type: ignore\n",
    ")\n",
    "val_dataset = val_dataset.with_transform(transform)\n",
    "test_dataset = datasets.load_from_disk(\n",
    "    dataset_dir + \"test-set\" # type: ignore\n",
    ")\n",
    "test_dataset = test_dataset.with_transform(transform)\n",
    "num_samples = 0\n",
    "\n",
    "# for idx, sample in enumerate(tqdm(dataset)):\n",
    "#     output = {key: value.shape if isinstance(value, torch.Tensor) else value for key, value in sample.items()}\n",
    "#     print(output)\n",
    "\n",
    "#     if idx >= num_samples:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gradio\n",
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     'wit_idx': 502620,\n",
    "#     'wikipedia_caption_image': torch.Size([3, 224, 224]),\n",
    "#     'wikipedia_text': '<section_title> Station layout </section_title>',\n",
    "#     'youtube_video_id': '6e7RO-o6u6w',\n",
    "#     'youtube_content_video': torch.Size([10, 3, 224, 224]),\n",
    "#     'youtube_content_audio': torch.Size([16000]),\n",
    "#     'youtube_description_text': \"<ysub> it's open somebody's gonna be building something there so the neighborhood\n",
    "# isn't is in change i don't think shintomicho has much of a personality when they took away the kabuki theater it\n",
    "# really did change  </ysub>\"\n",
    "# }\n",
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "from importlib.resources import path\n",
    "import gradio as gr\n",
    "import torchvision\n",
    "import torchaudio\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset,\n",
    "}\n",
    "\n",
    "\n",
    "def update_length_options(set_name):\n",
    "    max_idx = len(dataset_dict[set_name]) - 1\n",
    "    return gr.update(minimum=0, maximum=max_idx, step=1)\n",
    "\n",
    "\n",
    "def get_random_sample(set_name):\n",
    "    dataset = dataset_dict[set_name]\n",
    "    sample_index = random.randint(0, len(dataset) - 1)\n",
    "    return sample_index\n",
    "\n",
    "\n",
    "def generate_caption_output(caption_dict):\n",
    "    with gr.Column() as output:\n",
    "        for language_key, language_captions in caption_dict.items():\n",
    "            with gr.Tab(language_key):\n",
    "                for caption_key, caption in language_captions.items():\n",
    "                    gr.Textbox(value=caption, label=caption_key)\n",
    "\n",
    "    return gr.update(children=[output])\n",
    "\n",
    "\n",
    "def update_captions(language, set_name, sample_index):\n",
    "    dataset = dataset_dict[set_name]\n",
    "    sample = dataset[int(sample_index)]\n",
    "    caption_dict = sample[\"captions\"][language]\n",
    "\n",
    "    for key in [\n",
    "        \"caption_alt_text_description\",\n",
    "        \"caption_reference_description\",\n",
    "        \"caption_title_and_reference_description\",\n",
    "        \"context_page_description\",\n",
    "        \"context_section_description\",\n",
    "        \"hierarchical_section_title\",\n",
    "        \"page_title\",\n",
    "        \"section_title\",\n",
    "    ]:\n",
    "        if key not in caption_dict:\n",
    "            caption_dict[key] = \"<Unavailable/>\"\n",
    "\n",
    "    return [\n",
    "        gr.update(value=caption_dict[\"caption_alt_text_description\"]),\n",
    "        gr.update(value=caption_dict[\"caption_reference_description\"]),\n",
    "        gr.update(\n",
    "            value=caption_dict[\"caption_title_and_reference_description\"]\n",
    "        ),\n",
    "        gr.update(value=caption_dict[\"context_page_description\"]),\n",
    "        gr.update(value=caption_dict[\"context_section_description\"]),\n",
    "        gr.update(value=caption_dict[\"hierarchical_section_title\"]),\n",
    "        gr.update(value=caption_dict[\"page_title\"]),\n",
    "        gr.update(value=caption_dict[\"section_title\"]),\n",
    "    ]\n",
    "\n",
    "\n",
    "def update_language_choices(set_name, sample_index):\n",
    "    print(dataset_dict[set_name][int(sample_index)])\n",
    "    languages = list(\n",
    "        dataset_dict[set_name][int(sample_index)][\"captions\"].keys()\n",
    "    )\n",
    "    return gr.update(choices=languages, value=languages[0]), *update_captions(\n",
    "        languages[0], set_name, sample_index\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sample(set_name, sample_index):\n",
    "    # Load the dataset based on the set name (you'll need to implement this part)\n",
    "    dataset = dataset_dict[set_name]\n",
    "\n",
    "    # Retrieve the sample at the given index\n",
    "    sample = dataset[int(sample_index)]\n",
    "    # Extract the text, image, video, and audio from the sample (you'll need to adapt this to your specific dataset)\n",
    "    print(sample)\n",
    "    subtitles = sample[\"youtube_description_text\"]\n",
    "    wit_image = (\n",
    "        sample[\"wikipedia_caption_image\"].squeeze().permute(1, 2, 0).numpy()\n",
    "    )\n",
    "    youtube_image = (\n",
    "        sample[\"youtube_random_video_sample_image\"]\n",
    "        .squeeze()\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    video = (\n",
    "        sample[\"youtube_content_video\"].squeeze().permute(0, 2, 3, 1).numpy()\n",
    "        * 255\n",
    "    )\n",
    "    audio = sample[\"youtube_content_audio\"]\n",
    "\n",
    "    video_path = f\"../demo/temp_data/video-{set_name}-{sample_index}.mp4\"\n",
    "    audio_path = f\"../demo/temp_data/audio-{set_name}-{sample_index}.mp3\"\n",
    "    if not pathlib.Path(video_path).parent.exists():\n",
    "        pathlib.Path(video_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(audio.shape)\n",
    "    if not pathlib.Path(video_path).exists():\n",
    "        torchvision.io.write_video(video_path, video, fps=20)\n",
    "    if not pathlib.Path(audio_path).exists():\n",
    "        torchaudio.save(audio_path, audio.view(-1).unsqueeze(0), 16000)\n",
    "    return (\n",
    "        *update_language_choices(set_name=set_name, sample_index=sample_index),\n",
    "        subtitles,\n",
    "        wit_image,\n",
    "        youtube_image,\n",
    "        video_path,\n",
    "        audio_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_random_sample(set_name):\n",
    "    sample_idx = get_random_sample(set_name)\n",
    "    return gr.update(value=sample_idx), *load_sample(set_name, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "\n",
    "# for set_name in dataset_dict.keys():\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "#         with tqdm(total=len(dataset_dict[set_name])) as pbar:\n",
    "#             for _ in executor.map(load_sample, [set_name] * len(dataset_dict[set_name]), range(len(dataset_dict[set_name]))):\n",
    "#                 pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # TALI (Temporally and semantically Aligned Audio, Language and Images) Dataset Demo v-0.3.0 🖼️ 🔊 🎦 📝\n",
    "    ## What should I expect to see here? \n",
    "    This demo is intended to show you what the TALI dataset looks like. It is a dataset that used the [Wikipedia Image Text (WIT)](https://huggingface.co/datasets/wikimedia/wit_base) captions and article titles to search Youtube for videos that match the captions, and then subsquently downloads the video, audio, and subtitles from such videos.\n",
    "    The result is a rich multi modal dataset that has multiple caption types related to both the WiT Images, and the Youtube videos. This means learning can take place between either temporally or semantically aligned text, images, audio and video.\n",
    "    ## How was this collected?\n",
    "    1. We start from the [WiT dataset](https://huggingface.co/datasets/wikimedia/wit_base) use either the context_page_description or page_title, which we refer to as source-query, search youtube with it. Return top 100 result titles.\n",
    "    2. Compare the returning titles, which we'll call youtube-titles, with the source-query using the CLIP text embeddings of the largest CLIP model (patch-14, large) we had available whose alignments were the closest we've seen with human perception.\n",
    "    3. Choose top-1 title’s video based on the CLIP ranking.\n",
    "    4. Download video, break into 30 second segments. Apply CLIP image embedding to the first image of each segment, and compare with the video’s title text. Rank the segments based on this distance.\n",
    "    5. Choose the top-10 segments for each video. Extract image, audio and subtitle frames.\n",
    "    At sampling time:\n",
    "    Randomly select one of these 10 segments, choose a 10 second segment out of the 30 second clip. Return 200 video frames (spread throughout the 10 second segment), and, 160000 audio frames (10 seconds).\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    ### First select the set to sample from, and the index of the sample to load, and click the \"Fetch sample\" button OR simply click the \"Fetch random sample\" button to load a random sample.\n",
    "    \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_set_name = gr.Dropdown(\n",
    "                choices=[\"train\", \"val\", \"test\"],\n",
    "                value=\"train\",\n",
    "                label=\"Set name\",\n",
    "                info=\"Select the set to sample from\",\n",
    "            )\n",
    "        with gr.Column():\n",
    "            input_sample_index = gr.Slider(\n",
    "                minimum=0,\n",
    "                maximum=100,\n",
    "                randomize=True,\n",
    "                step=1,\n",
    "                interactive=True,\n",
    "                label=\"Datapoint idx to sample\",\n",
    "                info=\"Select the idx to sample\",\n",
    "            )\n",
    "        with gr.Column():\n",
    "            fetch_btn = gr.Button(\"Fetch sample\")\n",
    "            fetch_random_btn = gr.Button(\"Fetch random sample\")\n",
    "\n",
    "    input_set_name.change(\n",
    "        update_length_options, input_set_name, input_sample_index\n",
    "    )\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    ### The wikipedia image is semantically aligned to the youtube components, while the youtube components are temporally aligned to each other.\n",
    "    \"\"\"\n",
    "    )\n",
    "    output_subtitle = gr.Text(label=\"Youtube Subtitles\")\n",
    "    caption_title_and_reference_description = gr.Textbox(\n",
    "        label=\"caption_title_and_reference_description\"\n",
    "    )\n",
    "    page_title = gr.Textbox(label=\"page_title\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            output_wit_image = gr.Image(label=\"Wikipedia Image\")\n",
    "        with gr.Column():\n",
    "            output_youtube_image = gr.Image(label=\"Youtube Image\")\n",
    "        with gr.Column():\n",
    "            output_video = gr.Video(label=\"Youtube Video\")\n",
    "        with gr.Column():\n",
    "            output_audio = gr.Audio(label=\"Youtube Audio\")\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    ### Choose what language to display captions in (the captions are in multiple languages)\n",
    "    \"\"\"\n",
    "    )\n",
    "    output_language = gr.Dropdown(label=\"Wiki language ID\")\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    ### These captions are semantically aligned to the wikipedia image, and should ideally be semantically aligned to the youtube components, however the dataset was selected automatically and this is not always the case. Overall however, the captions are very good at describing the youtube components.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        section_title = gr.Textbox(label=\"section_title\")\n",
    "        hierarchical_section_title = gr.Textbox(\n",
    "            label=\"hierarchical_section_title\"\n",
    "        )\n",
    "    with gr.Row():\n",
    "        caption_alt_text_description = gr.Textbox(\n",
    "            label=\"caption_alt_text_description\"\n",
    "        )\n",
    "        caption_reference_description = gr.Textbox(\n",
    "            label=\"caption_reference_description\"\n",
    "        )\n",
    "    with gr.Row():\n",
    "        context_section_description = gr.Textbox(\n",
    "            label=\"context_section_description\"\n",
    "        )\n",
    "        context_page_description = gr.Textbox(label=\"context_page_description\")\n",
    "\n",
    "    output_language.change(\n",
    "        update_captions,\n",
    "        [output_language, input_set_name, input_sample_index],\n",
    "        [\n",
    "            caption_alt_text_description,\n",
    "            caption_reference_description,\n",
    "            caption_title_and_reference_description,\n",
    "            context_page_description,\n",
    "            context_section_description,\n",
    "            hierarchical_section_title,\n",
    "            page_title,\n",
    "            section_title,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    report_textbox = gr.Textbox(\n",
    "        info=\"Please describe the issue you found with the sample\",\n",
    "        label=\"Issue description\",\n",
    "    )\n",
    "    callback.setup(\n",
    "        [input_set_name, input_sample_index, output_language, report_textbox],\n",
    "        \"flagged_data_points\",\n",
    "    )\n",
    "    report_button = gr.Button(\n",
    "        \"Report Issue\", info=\"Report an issue with the sample\"\n",
    "    )\n",
    "    report_button.click(\n",
    "        lambda *args: callback.flag(args),\n",
    "        [input_set_name, input_sample_index, output_language, report_textbox],\n",
    "        None,\n",
    "        preprocess=False,\n",
    "    )\n",
    "    report_button.click(\n",
    "        lambda x, y: [\n",
    "            gr.update(\n",
    "                value=\"Issue has been reported. Thank you for your help!\",\n",
    "                interactive=False,\n",
    "            ),\n",
    "            gr.update(\n",
    "                value=\"Issue has been reported. Thank you for your help!\",\n",
    "                visible=False,\n",
    "            ),\n",
    "        ],\n",
    "        [report_button, report_textbox],\n",
    "        [report_button, report_textbox],\n",
    "    )\n",
    "\n",
    "    # fetch_random_btn.click(update_language_choices, [input_set_name, input_sample_index], [output_language, caption_alt_text_description, caption_reference_description, caption_title_and_reference_description, context_page_description, context_section_description, hierarchical_section_title, page_title, section_title])\n",
    "    fetch_random_btn.click(\n",
    "        fn=load_random_sample,\n",
    "        inputs=[input_set_name],\n",
    "        outputs=[\n",
    "            input_sample_index,\n",
    "            output_language,\n",
    "            caption_alt_text_description,\n",
    "            caption_reference_description,\n",
    "            caption_title_and_reference_description,\n",
    "            context_page_description,\n",
    "            context_section_description,\n",
    "            hierarchical_section_title,\n",
    "            page_title,\n",
    "            section_title,\n",
    "            output_subtitle,\n",
    "            output_wit_image,\n",
    "            output_youtube_image,\n",
    "            output_video,\n",
    "            output_audio,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # fetch_btn.click(update_language_choices, [input_set_name, input_sample_index], [output_language, caption_alt_text_description, caption_reference_description, caption_title_and_reference_description, context_page_description, context_section_description, hierarchical_section_title, page_title, section_title])\n",
    "    fetch_btn.click(\n",
    "        fn=load_sample,\n",
    "        inputs=[input_set_name, input_sample_index],\n",
    "        outputs=[\n",
    "            output_language,\n",
    "            caption_alt_text_description,\n",
    "            caption_reference_description,\n",
    "            caption_title_and_reference_description,\n",
    "            context_page_description,\n",
    "            context_section_description,\n",
    "            hierarchical_section_title,\n",
    "            page_title,\n",
    "            section_title,\n",
    "            output_subtitle,\n",
    "            output_wit_image,\n",
    "            output_youtube_image,\n",
    "            output_video,\n",
    "            output_audio,\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://bcfdf4d126673b9fe3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bcfdf4d126673b9fe3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/data_fast/tali-v-3-4captions.parquet/relevance/433/433967/0/muJhiJbWa-s/captions.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/evolvingfungus/forge/workspaces/tali_wit/tali_wit/data_plus.py\", line 659, in __call__\n",
      "    subtitle_dict=load_json(\n",
      "  File \"/home/evolvingfungus/forge/workspaces/tali_wit/tali_wit/utils.py\", line 221, in load_json\n",
      "    with open(filepath, \"rb\") as json_file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/data_fast/tali-v-3-4captions.parquet/relevance/433/433967/0/muJhiJbWa-s/captions.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_811305/904129266.py\", line 134, in load_random_sample\n",
      "    return gr.update(value=sample_idx), *load_sample(set_name, sample_idx)\n",
      "  File \"/tmp/ipykernel_811305/904129266.py\", line 97, in load_sample\n",
      "    subtitles = sample[\"youtube_description_text\"]\n",
      "KeyError: 'youtube_description_text'\n",
      "[Errno 2] No such file or directory: '/data_fast/tali-v-3-4captions.parquet/relevance/473/473778/0/ZmF4W8_szkE/captions.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/evolvingfungus/forge/workspaces/tali_wit/tali_wit/data_plus.py\", line 659, in __call__\n",
      "    subtitle_dict=load_json(\n",
      "  File \"/home/evolvingfungus/forge/workspaces/tali_wit/tali_wit/utils.py\", line 221, in load_json\n",
      "    with open(filepath, \"rb\") as json_file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/data_fast/tali-v-3-4captions.parquet/relevance/473/473778/0/ZmF4W8_szkE/captions.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/gradio/blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_811305/904129266.py\", line 134, in load_random_sample\n",
      "    return gr.update(value=sample_idx), *load_sample(set_name, sample_idx)\n",
      "  File \"/tmp/ipykernel_811305/904129266.py\", line 97, in load_sample\n",
      "    subtitles = sample[\"youtube_description_text\"]\n",
      "KeyError: 'youtube_description_text'\n"
     ]
    }
   ],
   "source": [
    "demo.queue(concurrency_count=8)\n",
    "demo.launch(share=True, debug=True, enable_queue=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8be86470e38501aa34785446b144512d347203ba2fe09ff4115a3f561b2ac78b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
