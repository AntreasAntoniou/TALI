{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f9fa6dfe290>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from rich import print\n",
    "from rich.traceback import install\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/evolvingfungus/miniconda/envs/minimal-ml-template/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ! pip install -e ../\n",
    "# ! mamba install -c conda-forge itables pandas -y\n",
    "# ! pip install pytorchvideo\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from math import floor\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from hydra_zen import builds, instantiate\n",
    "from PIL import Image\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision.transforms import Compose, RandomCrop, Resize, ToTensor\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from traitlets import default\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import datasets\n",
    "from tali_wit.data import (\n",
    "    AnyModalSample,\n",
    "    dataclass_collate,\n",
    "    default_image_transforms,\n",
    "    ModalityTypes,\n",
    "    select_subtitles_between_timestamps,\n",
    "    TALIDataset,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.decorators import configurable\n",
    "from tali_wit.utils import get_logger, load_json, save_json\n",
    "from tali_wit.models import ModalityConfig\n",
    "\n",
    "logger = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tali_wit.data_plus import TALIBaseDemoTransform, TALIBaseTransformConfig\n",
    "\n",
    "\n",
    "transform = TALIBaseDemoTransform(\n",
    "    config=TALIBaseTransformConfig(\n",
    "        root_filepath=\"/data/datasets/tali-wit-2-1-buckets/\",\n",
    "        modality_list=[\n",
    "            ModalityTypes.wit_image.value,\n",
    "            ModalityTypes.wit_caption.value,\n",
    "            ModalityTypes.wit_title.value,\n",
    "            ModalityTypes.wit_main_body.value,\n",
    "            ModalityTypes.youtube_image.value,\n",
    "            ModalityTypes.youtube_video.value,\n",
    "            ModalityTypes.youtube_subtitles.value,\n",
    "            ModalityTypes.youtube_audio.value,\n",
    "            ModalityTypes.youtube_description.value,\n",
    "        ],\n",
    "        rng_seed=42,\n",
    "        top_k_tali=10,\n",
    "        image_size=224,\n",
    "        num_video_frames=200,\n",
    "        num_audio_frames=160000,\n",
    "        clip_duration_in_seconds=10.0,\n",
    "    )\n",
    ")\n",
    "train_dataset = datasets.load_from_disk(\"/home/evolvingfungus/forge/workspaces/tali-2-2/train-set\")\n",
    "train_dataset = train_dataset.with_transform(transform)\n",
    "val_dataset = datasets.load_from_disk(\"/home/evolvingfungus/forge/workspaces/tali-2-2/val-set\")\n",
    "val_dataset = val_dataset.with_transform(transform)\n",
    "test_dataset = datasets.load_from_disk(\"/home/evolvingfungus/forge/workspaces/tali-2-2/test-set\")\n",
    "test_dataset = test_dataset.with_transform(transform)\n",
    "num_samples = 0\n",
    "\n",
    "# for idx, sample in enumerate(tqdm(dataset)):\n",
    "#     output = {key: value.shape if isinstance(value, torch.Tensor) else value for key, value in sample.items()}\n",
    "#     print(output)\n",
    "        \n",
    "#     if idx >= num_samples:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# {\n",
    "#     'wit_idx': 502620,\n",
    "#     'wikipedia_caption_image': torch.Size([3, 224, 224]),\n",
    "#     'wikipedia_text': '<section_title> Station layout </section_title>',\n",
    "#     'youtube_video_id': '6e7RO-o6u6w',\n",
    "#     'youtube_content_video': torch.Size([10, 3, 224, 224]),\n",
    "#     'youtube_content_audio': torch.Size([16000]),\n",
    "#     'youtube_description_text': \"<ysub> it's open somebody's gonna be building something there so the neighborhood \n",
    "# isn't is in change i don't think shintomicho has much of a personality when they took away the kabuki theater it \n",
    "# really did change  </ysub>\"\n",
    "# }\n",
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "import gradio as gr\n",
    "import torchvision\n",
    "import torchaudio\n",
    "\n",
    "dataset_dict = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "def update_length_options(set_name):\n",
    "    max_idx = len(dataset_dict[set_name])\n",
    "    return gr.update(minimum=0, maximum=max_idx, step=1)\n",
    "\n",
    "def generate_caption_output(caption_dict):\n",
    "    with gr.Column() as output:\n",
    "        for language_key, language_captions in caption_dict.items():\n",
    "            with gr.Tab(language_key):\n",
    "                for caption_key, caption in language_captions.items():\n",
    "                    gr.Textbox(value=caption, label=caption_key)\n",
    "                \n",
    "    return gr.update(children=[output])\n",
    "\n",
    "def update_captions(language, set_name, sample_index):\n",
    "    dataset = dataset_dict[set_name]\n",
    "    sample = dataset[int(sample_index)]\n",
    "    caption_dict = sample[\"captions\"][language]\n",
    "    \n",
    "    for key in [ \"caption_alt_text_description\",\n",
    "                \"caption_reference_description\",\n",
    "                \"caption_title_and_reference_description\",\n",
    "                \"context_page_description\",\n",
    "                \"context_section_description\",\n",
    "                \"hierarchical_section_title\",\n",
    "                \"page_title\",\n",
    "                \"section_title\",\n",
    "            ]:\n",
    "        if key not in caption_dict:\n",
    "            caption_dict[key] = \"<Unavailable/>\"\n",
    "    \n",
    "    \n",
    "    return [gr.update(value=caption_dict[\"caption_alt_text_description\"]), \n",
    "            gr.update(value=caption_dict[\"caption_reference_description\"]),\n",
    "            gr.update(value=caption_dict[\"caption_title_and_reference_description\"]),\n",
    "            gr.update(value=caption_dict[\"context_page_description\"]),\n",
    "            gr.update(value=caption_dict[\"context_section_description\"]),\n",
    "            gr.update(value=caption_dict[\"hierarchical_section_title\"]),\n",
    "            gr.update(value=caption_dict[\"page_title\"]),\n",
    "            gr.update(value=caption_dict[\"section_title\"])]\n",
    "def update_language_choices(set_name, sample_index):\n",
    "    return gr.update(choices=list(dataset_dict[set_name][int(sample_index)][\"captions\"].keys()))\n",
    "def load_sample(set_name, sample_index):\n",
    "    # Load the dataset based on the set name (you'll need to implement this part)\n",
    "    dataset = dataset_dict[set_name]\n",
    "\n",
    "    # Retrieve the sample at the given index\n",
    "    sample = dataset[int(sample_index)]\n",
    "\n",
    "    # Extract the text, image, video, and audio from the sample (you'll need to adapt this to your specific dataset)\n",
    "    languages = list(sample[\"captions\"].keys())\n",
    "    caption = sample[\"captions\"]\n",
    "    subtitles = sample[\"youtube_description_text\"]\n",
    "    wit_image = sample[\"wikipedia_caption_image\"].permute(1, 2, 0).numpy()\n",
    "    youtube_image = sample[\"youtube_random_video_sample_image\"].permute(1, 2, 0).numpy()\n",
    "    video = sample[\"youtube_content_video\"].permute(0, 2, 3, 1).numpy() * 255\n",
    "    audio = sample[\"youtube_content_audio\"]\n",
    "    \n",
    "    video_path = f\"temp_data/video-{set_name}-{sample_index}.mp4\"\n",
    "    audio_path = f\"temp_data/audio-{set_name}-{sample_index}.mp3\"\n",
    "    \n",
    "    if not pathlib.Path(\"temp_data/\").exists():\n",
    "        pathlib.Path(\"temp_data/\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not pathlib.Path(video_path).exists():\n",
    "        torchvision.io.write_video(video_path, video, fps=25)\n",
    "    if not pathlib.Path(audio_path).exists():\n",
    "        torchaudio.save(audio_path, audio.unsqueeze(0), 16000)\n",
    "    return \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", subtitles, wit_image, youtube_image, video_path, audio_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # TALI Dataset Demo\n",
    "    \"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_set_name = gr.Dropdown([\"train\", \"val\", \"test\"], label=\"Set name\", info=\"Select the set to sample from\")\n",
    "        with gr.Column():\n",
    "            input_sample_index = gr.Slider(minimum=0, maximum=100, step=1, interactive=True,\n",
    "                label=\"Datapoint idx to sample\",\n",
    "                info=\"Select the idx to sample\",\n",
    "            )\n",
    "    input_sample_index.change(update_length_options, input_set_name, input_sample_index)\n",
    "    fetch_btn = gr.Button(\"Fetch sample\")\n",
    "    \n",
    "    output_language = gr.Dropdown(label=\"Wiki language ID\")\n",
    "    with gr.Row():\n",
    "        caption_alt_text_description = gr.Textbox(label=\"caption_alt_text_description\")\n",
    "        caption_reference_description = gr.Textbox(label=\"caption_reference_description\")\n",
    "        caption_title_and_reference_description = gr.Textbox(label=\"caption_title_and_reference_description\")\n",
    "        context_page_description = gr.Textbox(label=\"context_page_description\")\n",
    "        context_section_description = gr.Textbox(label=\"context_section_description\")\n",
    "        hierarchical_section_title = gr.Textbox(label=\"hierarchical_section_title\")\n",
    "        page_title = gr.Textbox(label=\"page_title\")\n",
    "        section_title = gr.Textbox(label=\"section_title\")\n",
    "    output_language.select(update_captions, [output_language, input_set_name, input_sample_index], [caption_alt_text_description, caption_reference_description, caption_title_and_reference_description, context_page_description, context_section_description, hierarchical_section_title, page_title, section_title])\n",
    "    \n",
    "    output_subtitle = gr.Text(label=\"Youtube Subtitles\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            output_wit_image = gr.Image(label=\"Wikipedia Image\")\n",
    "        with gr.Column():\n",
    "            output_youtube_image = gr.Image(label=\"Youtube Image\")\n",
    "        with gr.Column():\n",
    "            output_video = gr.Video(label=\"Youtube Video\")\n",
    "        with gr.Column():\n",
    "            output_audio = gr.Audio(label=\"Youtube Audio\")\n",
    "    \n",
    "    fetch_btn.click(update_language_choices, [input_set_name, input_sample_index], output_language)\n",
    "    fetch_btn.click(fn=load_sample,\n",
    "    inputs=[input_set_name, input_sample_index],\n",
    "    outputs=[caption_alt_text_description,\n",
    "            caption_reference_description,\n",
    "            caption_title_and_reference_description,\n",
    "            context_page_description,\n",
    "            context_section_description,\n",
    "            hierarchical_section_title,\n",
    "            page_title,\n",
    "            section_title,\n",
    "            output_subtitle, output_wit_image, output_youtube_image, output_video, output_audio],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f5354eb614ae618570.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f5354eb614ae618570.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo.queue()\n",
    "demo.launch(share=True, debug=True, enable_queue=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8be86470e38501aa34785446b144512d347203ba2fe09ff4115a3f561b2ac78b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
