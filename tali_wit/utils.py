from functools import wraps
import logging
import shutil
import signal

import torch
from omegaconf import DictConfig, OmegaConf
from rich.logging import RichHandler
from rich.syntax import Syntax
from rich.traceback import install
from rich.tree import Tree

from huggingface_hub import (
    create_repo,
    hf_hub_download,
    login,
    snapshot_download,
)
import accelerate


def get_logger(
    name=__name__, logging_level: str = None, set_rich: bool = False
) -> logging.Logger:
    """Initializes multi-GPU-friendly python command line logger."""

    logger = logging.getLogger(name)

    logging_level = logging_level or logging.INFO

    logger.setLevel(logging_level)

    if set_rich:
        ch = RichHandler()

        # create formatter
        formatter = logging.Formatter("%(message)s")

        # add formatter to ch
        ch.setFormatter(formatter)

        # add ch to logger
        logger.addHandler(ch)

    install()

    # this ensures all logging levels get marked with the rank zero decorator
    # otherwise logs would get multiplied for each GPU process in multi-GPU setup

    return logger


def get_hydra_config(logger_level: str = "INFO"):
    return dict(
        job_logging=dict(
            version=1,
            formatters=dict(
                simple=dict(
                    level=logger_level,
                    format="%(message)s",
                    datefmt="[%X]",
                )
            ),
            handlers=dict(
                rich={
                    "class": "rich.logging.RichHandler",
                    # "formatter": "simple",
                }
            ),
            root={"handlers": ["rich"], "level": logger_level},
            disable_existing_loggers=False,
        ),
        hydra_logging=dict(
            version=1,
            formatters=dict(
                simple=dict(
                    level=logging.CRITICAL,
                    format="%(message)s",
                    datefmt="[%X]",
                )
            ),
            handlers={
                "rich": {
                    "class": "rich.logging.RichHandler",
                    # "formatter": "simple",
                }
            },
            root={"handlers": ["rich"], "level": logging.CRITICAL},
            disable_existing_loggers=False,
        ),
        run={
            "dir": "${current_experiment_dir}/hydra-run/${now:%Y-%m-%d_%H-%M-%S}"
        },
        sweep={
            "dir": "${current_experiment_dir}/hydra-multirun/${now:%Y-%m-%d_%H-%M-%S}",
            "subdir": "${hydra.job.num}",
        },
    )


def timeout(timeout_secs: int):
    def wrapper(func):
        @wraps(func)
        def time_limited(*args, **kwargs):
            # Register an handler for the timeout
            def handler(signum, frame):
                raise Exception(f"Timeout for function '{func.__name__}'")

            # Register the signal function handler
            signal.signal(signal.SIGALRM, handler)

            # Define a timeout for your function
            signal.alarm(timeout_secs)

            result = None
            try:
                result = func(*args, **kwargs)
            except Exception as exc:
                logging.error(f"Exploded due to time out on {args, kwargs}")
                raise exc
            finally:
                # disable the signal alarm
                signal.alarm(0)

            return result

        return time_limited

    return wrapper


def demo_logger():
    logger = get_logger(__name__)

    logger.info("Hello World")
    logger.debug("Debugging")
    logger.warning("Warning")
    logger.error("Error")
    logger.critical("Critical")
    logger.exception("Exception")


def set_seed(seed: int):
    accelerate.utils.set_seed(seed)


def pretty_config(
    config: DictConfig,
    resolve: bool = True,
):
    """Prints content of DictConfig using Rich library and its tree structure.

    Args:
        config (DictConfig): Configuration composed by Hydra.
        fields (Sequence[str], optional): Determines which main fields from config will
        be printed and in what order.
        resolve (bool, optional): Whether to resolve reference fields of DictConfig.
    """

    style = "dim"
    tree = Tree("CONFIG", style=style, guide_style=style)

    for field in config.keys():
        branch = tree.add(field, style=style, guide_style=style)

        config_section = config.get(field)
        branch_content = str(config_section)
        if isinstance(config_section, DictConfig):
            branch_content = OmegaConf.to_yaml(config_section, resolve=resolve)

        branch.add(Syntax(branch_content, "yaml"))

    return tree


import os
import os.path
import pathlib
from typing import Any, Dict, Union

import orjson as json


def save_json(
    filepath: Union[str, pathlib.Path], dict_to_store: Dict, overwrite=True
):
    """
    Saves a metrics .json file with the metrics
    :param log_dir: Directory of log
    :param metrics_file_name: Name of .csv file
    :param dict_to_store: A dict of metrics to add in the file
    :param overwrite: If True overwrites any existing files with the same filepath,
    if False adds metrics to existing
    """

    if isinstance(filepath, str):
        filepath = pathlib.Path(filepath)

    if overwrite and filepath.exists():
        filepath.unlink(missing_ok=True)

    if not filepath.parent.exists():
        filepath.parent.mkdir(parents=True, exist_ok=True)

    with open(filepath, "wb") as json_file:
        json_file.write(json.dumps(dict_to_store))
    return filepath


def load_json(filepath: Union[str, pathlib.Path]):
    """
    Loads the metrics in a dictionary.
    :param log_dir: The directory in which the log is saved
    :param metrics_file_name: The name of the metrics file
    :return: A dict with the metrics
    """

    if isinstance(filepath, str):
        filepath = pathlib.Path(filepath)

    with open(filepath, "rb") as json_file:
        dict_to_load = json.loads(json_file.read())

    return dict_to_load


logger = get_logger(name=__name__)


def download_model_with_name(
    hf_repo_path, hf_cache_dir, model_name, download_only_if_finished=False
):
    if not pathlib.Path(
        pathlib.Path(hf_cache_dir) / "checkpoints" / f"{model_name}"
    ).exists():
        pathlib.Path(
            pathlib.Path(hf_cache_dir) / "checkpoints" / f"{model_name}"
        ).mkdir(parents=True, exist_ok=True)

    config_filepath = hf_hub_download(
        repo_id=hf_repo_path,
        cache_dir=pathlib.Path(hf_cache_dir),
        resume_download=True,
        filename="config.yaml",
        repo_type="model",
    )

    config_path = pathlib.Path(hf_cache_dir) / "config.yaml"

    shutil.copy(
        pathlib.Path(config_filepath),
        config_path,
    )

    trainer_state_filepath = hf_hub_download(
        repo_id=hf_repo_path,
        cache_dir=pathlib.Path(hf_cache_dir),
        resume_download=True,
        subfolder=f"checkpoints/{model_name}",
        filename="trainer_state.pt",
        repo_type="model",
    )

    trainer_path = (
        pathlib.Path(hf_cache_dir)
        / "checkpoints"
        / f"{model_name}"
        / "trainer_state.pt"
    )

    shutil.copy(
        pathlib.Path(trainer_state_filepath),
        trainer_path,
    )
    logger.info(
        f"Trainer state copied to {trainer_path} from {trainer_state_filepath}."
    )

    if download_only_if_finished:
        state_dict = torch.load(trainer_path)["state_dict"]["eval"][0][
            "auc-macro"
        ]
        global_step_list = list(state_dict.keys())
        if len(global_step_list) < 40:
            return False

    optimizer_filepath = hf_hub_download(
        repo_id=hf_repo_path,
        cache_dir=pathlib.Path(hf_cache_dir),
        resume_download=True,
        subfolder=f"checkpoints/{model_name}",
        filename="optimizer.bin",
        repo_type="model",
    )

    model_filepath = hf_hub_download(
        repo_id=hf_repo_path,
        cache_dir=pathlib.Path(hf_cache_dir),
        resume_download=True,
        subfolder=f"checkpoints/{model_name}",
        filename="pytorch_model.bin",
        repo_type="model",
    )

    random_states_filepath = hf_hub_download(
        repo_id=hf_repo_path,
        cache_dir=pathlib.Path(hf_cache_dir),
        resume_download=True,
        subfolder=f"checkpoints/{model_name}",
        filename="random_states_0.pkl",
        repo_type="model",
    )

    try:
        scaler_state_filepath = hf_hub_download(
            repo_id=hf_repo_path,
            cache_dir=pathlib.Path(hf_cache_dir),
            resume_download=True,
            subfolder=f"checkpoints/{model_name}",
            filename="scaler.pt",
            repo_type="model",
        )
    except Exception as e:
        scaler_state_filepath = None

    target_optimizer_path = (
        pathlib.Path(hf_cache_dir)
        / "checkpoints"
        / f"{model_name}"
        / "optimizer.bin"
    )

    shutil.copy(
        pathlib.Path(optimizer_filepath),
        target_optimizer_path,
    )

    target_model_path = (
        pathlib.Path(hf_cache_dir)
        / "checkpoints"
        / f"{model_name}"
        / "pytorch_model.bin"
    )

    shutil.copy(
        pathlib.Path(model_filepath),
        target_model_path,
    )

    random_states_path = (
        pathlib.Path(hf_cache_dir)
        / "checkpoints"
        / f"{model_name}"
        / "random_states_0.pkl"
    )

    shutil.copy(
        pathlib.Path(random_states_filepath),
        random_states_path,
    )

    if scaler_state_filepath is not None:
        scaler_path = (
            pathlib.Path(hf_cache_dir)
            / "checkpoints"
            / f"{model_name}"
            / "scaler.pt"
        )

        shutil.copy(
            pathlib.Path(scaler_state_filepath),
            scaler_path,
        )

    return {
        "root_filepath": pathlib.Path(hf_cache_dir)
        / "checkpoints"
        / f"{model_name}",
        "optimizer_filepath": target_optimizer_path,
        "model_filepath": target_model_path,
        "random_states_filepath": random_states_path,
        "trainer_state_filepath": trainer_path,
        "config_filepath": config_path,
    }


def create_hf_model_repo_and_download_maybe(cfg: Any):
    import yaml
    from huggingface_hub import HfApi

    if (
        cfg.download_checkpoint_with_name is not None
        and cfg.download_latest is True
    ):
        raise ValueError(
            "Cannot use both continue_from_checkpoint_with_name and continue_from_latest"
        )

    hf_repo_path = cfg.hf_repo_path
    login(token=os.environ["HF_TOKEN"], add_to_git_credential=True)
    print(
        f"Logged in to huggingface with token {os.environ['HF_TOKEN']}, creating repo {hf_repo_path}"
    )
    repo_url = create_repo(hf_repo_path, repo_type="model", exist_ok=True)

    logger.info(f"Created repo {hf_repo_path}, {cfg.hf_cache_dir}")

    if not pathlib.Path(cfg.hf_cache_dir).exists():
        pathlib.Path(cfg.hf_cache_dir).mkdir(parents=True, exist_ok=True)
        pathlib.Path(pathlib.Path(cfg.hf_cache_dir) / "checkpoints").mkdir(
            parents=True, exist_ok=True
        )

    config_dict = OmegaConf.to_container(cfg, resolve=True)

    hf_api = HfApi()

    config_json_path: pathlib.Path = save_json(
        filepath=pathlib.Path(cfg.hf_cache_dir) / "config.json",
        dict_to_store=config_dict,
        overwrite=True,
    )

    hf_api.upload_file(
        repo_id=hf_repo_path,
        path_or_fileobj=config_json_path.as_posix(),
        path_in_repo="config.json",
    )

    config_yaml_path = pathlib.Path(cfg.hf_cache_dir) / "config.yaml"
    with open(config_yaml_path, "w") as file:
        documents = yaml.dump(config_dict, file)

    hf_api.upload_file(
        repo_id=hf_repo_path,
        path_or_fileobj=config_yaml_path.as_posix(),
        path_in_repo="config.yaml",
    )

    try:
        if cfg.resume == False and not (
            cfg.download_checkpoint_with_name is not None or cfg.download_latest
        ):
            return None, repo_url

        if cfg.download_checkpoint_with_name is not None:
            logger.info(
                f"Download {cfg.download_checkpoint_with_name} checkpoint, if it exists, from the huggingface hub 👨🏻‍💻"
            )

            path_dict = download_model_with_name(
                hf_repo_path=hf_repo_path,
                hf_cache_dir=cfg.hf_cache_dir,
                model_name=cfg.download_checkpoint_with_name,
            )
            logger.info(
                f"Downloaded checkpoint from huggingface hub to {cfg.hf_cache_dir}"
            )
            return path_dict["root_filepath"], repo_url

        elif cfg.download_latest:
            files = hf_api.list_repo_files(
                repo_id=hf_repo_path,
            )

            ckpt_dict = {}
            for file in files:
                if "checkpoints/ckpt" in file:
                    ckpt_global_step = int(file.split("/")[-2].split("_")[-1])
                    ckpt_dict[ckpt_global_step] = "/".join(file.split("/")[:-1])

            latest_ckpt = ckpt_dict[max(ckpt_dict.keys())]

            model_dir = pathlib.Path(cfg.hf_cache_dir) / latest_ckpt
            if model_dir.exists():
                logger.info("Checkpoint exists, skipping download")
            else:
                logger.info(
                    "Download latest checkpoint, if it exists, from the huggingface hub 👨🏻‍💻"
                )
                path_dict = download_model_with_name(
                    model_name=latest_ckpt.split("/")[-1],
                    hf_repo_path=hf_repo_path,
                    hf_cache_dir=cfg.hf_cache_dir,
                )
                logger.info(
                    f"Downloaded checkpoint from huggingface hub to {cfg.hf_cache_dir}"
                )
            return (
                model_dir,
                repo_url,
            )
        else:
            logger.info(
                "Download all available checkpoints, if they exist, from the huggingface hub 👨🏻‍💻"
            )

            ckpt_folderpath = snapshot_download(
                repo_id=hf_repo_path,
                cache_dir=pathlib.Path(cfg.hf_cache_dir),
                resume_download=True,
            )
            latest_checkpoint = (
                pathlib.Path(cfg.hf_cache_dir) / "checkpoints" / "latest"
            )

            if pathlib.Path(
                pathlib.Path(cfg.hf_cache_dir) / "checkpoints"
            ).exists():
                pathlib.Path(
                    pathlib.Path(cfg.hf_cache_dir) / "checkpoints"
                ).mkdir(parents=True, exist_ok=True)

            shutil.copy(
                pathlib.Path(ckpt_folderpath), cfg.hf_cache_dir / "checkpoints"
            )

            if latest_checkpoint.exists():
                logger.info(
                    f"Downloaded checkpoint from huggingface hub to {latest_checkpoint}"
                )
            return cfg.hf_cache_dir / "checkpoints" / "latest"

    except Exception as e:
        return None, repo_url
